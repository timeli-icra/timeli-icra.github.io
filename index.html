<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="utf-8" />
  <meta name="description" content="Project / Paper description" />
  <meta name="keywords" content="navigation, dataset, accessibility" />
  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <title>Time-Aware Assistive Navigation</title>

  <!-- Local CSS (replace with your own files if needed) -->
  <link rel="stylesheet" href="./static/css/bulma.min.css" />
  <link rel="stylesheet" href="./static/css/index.css" />

  <meta property="og:site_name" content="{{PROJECT_NAME}}" />
  <meta property="og:type" content="website" />
  <meta property="og:title" content="{{TITLE}}" />

  <style>
    .video-and-annotations { display:flex; align-items:center; gap:1.5rem; flex-wrap:wrap; }
    .annotation-box { padding:12px; border-radius:6px; }
    .annotation-box.high-level { background:#6e6e6e; color:#fff; }
    .annotation-box.low-level { background:#e9e9e9; color:#111; }
    .annotation-title { font-size:1.05rem; font-weight:600; margin-bottom:0.4rem; }
    .publication-title { margin-bottom:0.5rem; }
    .publication-links .link-block { margin-right:0.5rem; display:inline-block; margin-bottom:0.4rem; }
    footer { margin-top:2.5rem; padding:1rem 0; }
  </style>
</head>
<body>
  <section class="hero">
    <div class="hero-body">
      <div class="container has-text-centered">
        <h1 class="title is-1 publication-title">Time-Aware Assistive Navigation</h1>
<!--         <div class="is-size-5 publication-authors">
          <span class="author-block">{{Anonymous Submission}}</span>
          <div class="brmod"></div>
        </div> -->

<!--         <div class="publication-links" style="margin-top:1rem;">
          <span class="link-block">
            <a href="./resources/paper.pdf" class="button is-normal is-rounded is-dark">
              <span>Paper</span>
            </a>
          </span>
          <span class="link-block">
            <a href="#video_section" class="button is-normal is-rounded is-dark">
              <span>Video</span>
            </a>
          </span>
          <span class="link-block">
            <a href="./resources/visualization.zip" class="button is-normal is-rounded is-dark">
              <span>Visualization</span>
            </a>
          </span>
          <span class="link-block">
            <a href="./resources/data_archive.zip" class="button is-normal is-rounded is-dark">
              <span>Data</span>
            </a>
          </span>
        </div> -->
      </div>
    </div>
  </section>

    
  <!-- Abstract -->
  <section class="section">
    <div class="container">
        <div class="columns is-centered has-text-centered">
            <div class="column is-two-thirds">
                <h2 class="title is-3">Abstract</h2>
                <div class="content has-text-justified">
                    <p>
                      Our TIMELI task goes beyond standard video-based summarization and indoor navigation benchmarks for MLLMs—we uniquely emphasize spatio-temporal coherence and planning in delivering continual language instructions under dynamic, end-user interactive settings. 
                      On this website, we provide additional details on: 
                      (1) <strong>benchmark construction</strong> and formulation, including data generation, annotation, and quality validation processes; 
                      (2) full <strong>prompt structure</strong> and experimental details, including metric definitions; 
                      (3) model ablations and additional <strong>results</strong>, including a preliminary validation in <strong>closed-loop real-world</strong> settings, 
                      leveraging a smartphone-based app instantiation (to be open-sourced) and generalization ablations on related <strong>real-world tasks</strong> to further study the benchmark's impact; and 
                      (4) additional <strong>qualitative results</strong>, including both success and failure cases.</p>
                </div>
            </div>
        </div>
    </div>
  </section>

  <!-- Video + Annotations -->
  <section class="section">
    <div class="container">
      <div class="columns is-centered">
        <div class="column is-two-thirds">

          <div class="columns video-and-annotations">
            <div class="column is-full">
              <div class="vis-video">
                <video controls muted loop style="width:100%">
                  <source src="./resources/TIMELI_supp.mp4" type="video/mp4" />
                </video>
              </div>
            </div>

            <div class="column is-full">
<!--               <h3 class="annotation-title">High-Level Annotation</h3>
              <div class="annotation-box high-level"> -->
                <div class="content">
                  <p>
                  This video is divided into four key segments:
                  <ul>
                    <li><strong>0:03 - 0:39:</strong> TIMELI Benchmark (with GT).</li>
                    <li><strong>0:40 - 1:12:</strong> ChatGPT-4o Performance.</li>
                    <li><strong>1:13 - 2:05:</strong> Finetuned Model Performance on Simulation Videos.</li>
                    <li><strong>2:06 - 2:57:</strong> Qualitative Results on Real-World Videos.</li>
                  </ul>
                </p>
                </div>
              </div>
            </div>
          </div>

          <!-- Duplicate above block for more videos/annotations -->

        </div>
      </div>
    </div>
  </section>


  <!-- Data / Statistics -->
<!--   <section class="section">
    <div class="container has-text-centered">
      <h2 class="title is-4">Data / Statistics</h2>
      <div class="column">
        <img src="./resources/data_statistic.png" alt="data statistics" style="max-width:100%;" />
      </div>
    </div>
  </section> -->

  <!-- Video section -->
<!--   <section id="video_section" class="section">
    <div class="container has-text-centered">
      <h2 class="title is-3">Video</h2>
      <div class="publication-video">
        <video controls style="max-width:100%;">
          <source src="./resources/supplementary_video.mp4" type="video/mp4" />
          Your browser does not support the video tag.
        </video>
      </div>
    </div>
  </section> -->

  <!-- Table of Contents -->
  <section class="section">
    <div class="container toc">
      <h2 class="title is-3">Table of Contents</h2>
      <ul>
        <li><a href="#sec1">1 TIMELI Benchmark Details</a>
          <ul>
            <li><a href="#sec1-1">1.1 Informed Instruction Design</a></li>
            <li><a href="#sec1-2">1.2 Adapting CARLA to Realistic Assistive Navigation Scenarios</a></li>
            <li><a href="#sec1-3">1.3 Real-World Video Annotation Process</a></li>
          </ul>
        </li>
        <li><a href="#sec2">2 Experimental Settings</a>
          <ul>
            <li><a href="#sec2-1">2.1 Prompt and Input Settings</a></li>
            <li><a href="#sec2-2">2.2 Metric Definitions</a></li>
            <li><a href="#sec2-3">2.3 Realistic Closed-Loop Evaluation with Noisy Conditions</a></li>
          </ul>
        </li>
        <li><a href="#sec3">3 Additional Experimental Analysis</a>
          <ul>
            <li><a href="#sec3-1">3.1 Results Summary and Error Intervals</a></li>
            <li><a href="#sec3-2">3.2 Model Components Ablation</a></li>
            <li><a href="#sec3-3">3.3 Safety-Critical Timing Analysis</a></li>
            <li><a href="#sec3-4">3.4 Inference Time</a></li>
            <li><a href="#sec3-5">3.5 Model Generalization to BDD</a></li>
            <li><a href="#sec3-6">3.6 Preliminary Smartphone-based Evaluation</a></li>
          </ul>
        </li>
        <li><a href="#sec4">4 Additional Qualitative Results</a></li>
      </ul>
    </div>
  </section>

  <!-- Sections -->
  <section class="section">
    <div class="container">
      <h2 id="sec1" class="title is-2">1 TIMELI Benchmark Details</h2>
      <h3 id="sec1-1" class="title is-4">1.1 Informed Instruction Design</h3>
      <p>
        TIMELI comprises a large-scale, synthetically generated (i.e., rule-based) instructional simulation and benchmark, along with a real-world, human-annotated benchmark. Both benchmarks undergo an iterative design and quality validation process.
        Our instruction design is based on real-world orientation and mobility (O&M) principles used to guide visually impaired individuals. Navigation through unfamiliar, complex environments remains a cognitively demanding and risky task, especially around intersections and moving obstacles.
      </p>
      <p>
        <strong>Iterative Design with Orientation and Mobility Guides:</strong>
        We initially performed a focus group with three local orientation and mobility guides regarding real-world scenarios and guidance design. 
        The Zoom session lasted two hours, and guides were paid for their time ($50 an hour). During the session, guides reported various support strategies, such as providing clock orientation, frequent affirmation at safety-critical tasks when just entering or exiting an intersection (i.e., junctions), 
        and maintaining silence around them. For transparency, we will release the transcripts from the sessions alongside the benchmark.
        Subsequently, we asked guides to remotely support blind participants in an in-situ, real-world study. 
        A recurrent theme involved only alerting users of the most salient environmental characteristics, defined at the span of the white cane. 
        While we focus on white cane users (the most frequent mobility aid used today), guides also mentioned that guide dog users could benefit from contextual information about the subset of off-ground objects in their path. 
        However, users generally offloaded object avoidance tasks to their guide dogs.
        While MLLMs seem to fail at basic instructional logic, we also emphasize that correct instruction requires a fine-grained and precise 3D understanding of scene context, 
        and the instruction set can be high-dimensional at times beyond basic classification of turn direction and objects, i.e., due to rich environmental descriptions that help navigators. 
        Moreover, we also observed indications for differing preferences and personalization, an aspect which we plan to study in the future.
      </p>
      <p>
        <strong>Synthetic Scenarios:</strong>
        Given the preliminary data, we designed simulated navigation scenarios in CARLA. 
        The simulated scenarios and corresponding instructions were shown to guides, and we discussed how each guide would change the scenario to increase realism and the instructions to adhere to orientation and mobility principles. 
        For instance, guides often would reflect on their own experience and mention how users can be hesitant in unfamiliar routes, taking smaller steps. Guides also mentioned how frequent but short instructions can help users stay on the path without getting overly distracted (e.g., “okay”, “a bit more”). 
        Moreover, while previous literature reports sighted guides should not instruct in junctions, guides reported several exceptions where short instructions are still needed to maintain orientation. We design these into our simulation as well.
        We find that off-the-shelf models struggle with our task, speaking repeatedly and at the wrong time while confusing directionality, distance, and relevant context. We now describe our automatic instruction generation procedure. 
        One guide described the difficulty in aligning to cross in certain junctions (e.g., in areas that lack clear pavements and curbs), while reiterating a common occurrence of significant veering during the crossing. 
        Thus, our instructions also incorporate short orientation feedback throughout the crossing in the case of slight deviation (above 30 degrees) from the planned path. 
        As a rule of thumb, we emphasize concise instruction at the correct time, defined either based on distance from landmarks or at fixed intervals.
        We leverage the rule-based generation to sample a random set of 1,000 clips, which are subsequently shown again to the sighted guides for quality validation and final refinements. Finally, the complete benchmark is generated.
      </p>

      <h3 id="sec1-2" class="title is-4">1.2 Adapting CARLA to Realistic Assistive Navigation Scenarios</h3>
      <p>
      We adapt the CARLA simulator to support rule-based instruction generation for assistive navigation. The simulation is informed by real-world studies and feedback from orientation and mobility experts. Pedestrians are spawned with egocentric sensors and randomized goals across different town and weather conditions.
      </p>
      
      <p>
      Since CARLA’s default pedestrian controller is limited (e.g., it sticks to the center of sidewalks and avoids nearby objects), we modify pedestrian paths to simulate realistic behaviors like veering and navigating close to environmental elements. This mirrors how blind individuals use tactile cues to explore their surroundings.
      </p>
      <h4 class="title is-5">Sampling Realistic Paths</h4>
      <p>
      We increase path diversity by sampling waypoints near buildings and curbs and adding noise to simulate veering. Walkable regions are extracted using semantic maps, and route planning is handled with A* search. These paths generate video data at 1 frame per second, along with metadata such as obstacle type, location, sensor readings, and weather.
      </p>
      
      <h4 class="title is-5">Instruction Generation</h4>
      <p>
      Instructions are generated using a Compound Action Specification (CAS) based on user orientation, route progress, and environment context. Each instruction includes:
      </p>
      <ul>
        <li><strong>Action:</strong> walk, turn</li>
        <li><strong>Direction:</strong> forward, left, right</li>
        <li><strong>Angle:</strong> slightly, or clock-face references</li>
      </ul>
      
      <p>
      We add context with nearby object or landmark descriptions (e.g., “building on the right”) using nine object types: building, fence, pedestrian, pole, vegetation, vehicle, wall, traffic sign. Relative directions are: in front, left, right.
      </p>
      
      <p>
      Objects within 1.5m (cane range) trigger instructions like:
      </p>
      
      <div class="box">
        <pre><code>
      Continue forward. Be careful, pole in front.
      Turn slightly left. Vehicle on your right.
        </code></pre>
      </div>
      
      <p>
      A bird’s-eye view (from segmentation and depth maps) helps calculate distances. We also predict pedestrian movement using a bicycle model to time cues in advance.
      </p>
      
      <h4 class="title is-5">Event-Driven Navigation Prompts</h4>
      <p>
      While on sidewalks, instructions are triggered when a turn is required or an obstacle is detected within 1.5m. For example:
      </p>
      <div class="box">
        <pre><code>
      Walk along the wall on your right.
        </code></pre>
      </div>
      
      <p>
      At junctions, minimal instruction is used to avoid distraction. Example prompts include:
      </p>
      <ul>
        <li>“Approaching junction in five meters.”</li>
        <li>“You are entering/crossing a junction.”</li>
        <li>“You have exited the junction.”</li>
      </ul>
      
      <p>
      We also log the reasoning behind issuing or withholding instructions, such as obstacle detection, path turning, or being inside a junction.
      </p>
      
      <figure class="image" style="max-width:800px; margin:0 auto; text-align:center;">
        <img src="./resources/supp1.png" 
             style="width:100%; height:auto; display:block; margin:0 auto;" 
             alt="Pedestrian Paths"> 
        <figcaption style="margin-top:0.5rem; font-size:1rem; color:#555;">
          <strong>Figure 1: Extracting Layout Details for Modified Pedestrian Paths.</strong>  We extract walkable and
          sidewalk space in the map, and employ it to sample waypoints nearby buildings and curbs. In
          the figure, red denotes sidewalk space and white and green denotes road or obstructed space. The
          sampled waypoints are then provided as intermediate goals to guide path planning for spawned
          pedestrians during data collection. This ensures a realistic and challenging benchmark.
        </figcaption>
      </figure>

      <h3 id="sec1-3" class="title is-4">1.3 Real-World Video Annotation Process</h3>
      <figure class="image" style="max-width:1200px; margin:0 auto; text-align:center;">
        <img src="./resources/supp3_annotation_interface.png" 
             style="width:100%; height:auto; display:block; margin:0 auto;" 
             alt="Pedestrian Paths"> 
        <figcaption style="margin-top:0.5rem; font-size:1rem; color:#555;">
          <strong>Figure 3: Annotation Interface for YouTube Videos</strong> We collect a diverse set of U.S.-based YouTube 
          videos and manually annotate them through the interface in a multi-round validation process.
        </figcaption>
      </figure>
      <p>...</p>

      <h2 id="sec2" class="title is-2">2 Experimental Settings</h2>
      <h3 id="sec2-1" class="title is-4">2.1 Prompt and Input Settings</h3>
      <h4 class="title is-5">Prompt Example</h4>
      <div class="box">
        <pre><code>
          You are guiding a blind person. The blind person needs to approach the goal
          [x,y]={goal coordinate} located in relative longitudinal and latitudinal offsets
          in meters.
          Generate an instruction for the current frame. You will need to provide step-by-
          step verbal navigation instructions to help them reach the destination safely and
          efficiently. Instruct the user to stay on the path to the goal, only notify what is
          needed, including immediate turns they need to make and obstacles to avoid
          within 1.5m (cane distance). Keep the instructions in junctions minimal for
          safety to avoid distraction as the user relies on hearing to navigate and listen
          to traffic. You should generally not instruct the user for consecutive frames
          unless absolutely needed, and avoid too frequent instructions. Only return the
          instruction to convey. Return “None” for remaining silent.
          
          Examples:
          Input: goal: [x,y]=[0,1], Output: Keep walking straight.
          Input: goal: [x,y]=[0,1], Output: None.
          Input: goal: [x,y]=[1,0], Output: Turn right and keep crossing.
          Input: goal: [x,y]=[0.5,0], Output: Turn slightly right and keep walking. There’s a pole on your left.
          Input: goal: [x,y]=[-1,0], Output: Turn to your nine and keep walking. There’s a pole on your right and vehicle on your left.
          Input: goal: [x,y]=[-0.5,0], Output: Turn slightly left and continue walking. There’s a vehicle on your left.
          Input: goal: [x,y]=[0,-1], Output: Turn around. There’s a fence on your left.
        </code></pre>
      </div>
  
      <p>...</p>

      <h3 id="sec2-2" class="title is-4">2.2 Metric Definitions</h3>
      <p>...</p>

      <h3 id="sec2-3" class="title is-4">2.3 Realistic Closed-Loop Evaluation with Noisy Conditions</h3>
      <p>...</p>

      <h2 id="sec3" class="title is-2">3 Additional Experimental Analysis</h2>
      <h3 id="sec3-1" class="title is-4">3.1 Results Summary and Error Intervals</h3>
      <p>...</p>

      <h3 id="sec3-2" class="title is-4">3.2 Model Components Ablation</h3>
      <p>...</p>

      <h3 id="sec3-3" class="title is-4">3.3 Safety-Critical Timing Analysis</h3>
      <p>...</p>

      <h3 id="sec3-4" class="title is-4">3.4 Inference Time</h3>
      <p>...</p>

      <h3 id="sec3-5" class="title is-4">3.5 Model Generalization to BDD</h3>
      <p>...</p>

      <h3 id="sec3-6" class="title is-4">3.6 Preliminary Smartphone-based Evaluation</h3>
      <p>...</p>

      <h2 id="sec4" class="title is-2">4 Additional Qualitative Results</h2>
      <p>...</p>


      

<!--       <figure class="image" style="max-width:800px; margin:0 auto; text-align:center;">
          <img src="./resources/supp2_statistics.png" 
               style="width:100%; height:auto; display:block; margin:0 auto;" 
               alt="Pedestrian Paths"> 
          <figcaption style="margin-top:0.5rem; font-size:1rem; color:#555;">
            <strong>Figure 2: Benchmark Statistics</strong> Left: distribution of events, categorized by whether the user is at
            an intersection. Right: keywords from instructional tasks, derived through in-situ study and trained
            mobility guides interviews.
          </figcaption>
        </figure> -->

      

      <figure class="image" style="max-width:800px; margin:0 auto; text-align:center;">
          <img src="./resources/supp4_piechart.png" 
               style="width:100%; height:auto; display:block; margin:0 auto;" 
               alt="Pedestrian Paths"> 
          <figcaption style="margin-top:0.5rem; font-size:1rem; color:#555;">
            <strong>Figure 4: Radar Plot for Fine-tuned Models in Simulation.</strong> We leverage the proposed video-based
            assistive navigation task to evaluate multiple dimensions of instruction generation models. 
            By finetuning models on our TIMELI benchmark, we observe that existing models frequently make errors in
            our navigation task, including timing of instructions (Timing), clarity and brevity of communication
            (Conciseness), identification of the reason (Instruction Reason), appropriate silence in intersections
            (Silence in Junction), accuracy in detecting relevant obstacles (Object Precision), and correctness of
            direction (Instruction Direction).
          </figcaption>
        </figure>

       <figure class="image" style="max-width:800px; margin:0 auto; text-align:center;">
          <img src="./resources/supp5_smartphone.png" 
               style="width:100%; height:auto; display:block; margin:0 auto;" 
               alt="Pedestrian Paths"> 
          <figcaption style="margin-top:0.5rem; font-size:1rem; color:#555;">
            <strong>Figure 5: iPhone App Interface</strong> We validate our model in closed-loop, real-world settings using an
            iPhone app instantiation. The app displays the front-view image on the left and the planned path to
            the destination (via Google Maps) on the right. Instructions are shown at the top and also delivered
            through audio.
          </figcaption>
        </figure>

       <figure class="image" style="max-width:800px; margin:0 auto; text-align:center;">
          <img src="./resources/supp6_closedloop_vis.png" 
               style="width:100%; height:auto; display:block; margin:0 auto;" 
               alt="Pedestrian Paths"> 
          <figcaption style="margin-top:0.5rem; font-size:1rem; color:#555;">
            <strong>Figure 6: Closed-loop Visualization.</strong> We present success and failure cases from the closed-loop
            navigation evaluation. The blue line shows the intended route, and the orange line shows the path a
            follower walks based on the instructions of a TIMELI-trained model. For failure cases, the model
            demonstrates an ability to recover from deviations by issuing corrective instructions. However, the
            recovery process often incurs additional time, which can lead to failure to reach the destination within
            the allotted time budget.
          </figcaption>
        </figure>

    </div>
  </section>

  <!-- Acknowledgments -->
<!--   <section class="section">
    <div class="container has-text-centered">
      <h2 class="title is-3">Acknowledgments</h2>
      <div class="content has-text-justified">
        <p>{{ACKNOWLEDGMENTS_PLACEHOLDER}}</p>
      </div>
    </div>
  </section> -->

  <!-- BibTeX -->
<!--   <section class="section">
    <div class="container has-text-centered">
      <h2 class="title is-3">BibTeX</h2>
      <pre><code>
@inproceedings{placeholder202X,
  title={{TITLE}},
  author={{AUTHORS_PLACEHOLDER}},
  booktitle={{CONFERENCE_PLACEHOLDER}},
  year={{YEAR_PLACEHOLDER}}
}
      </code></pre>
    </div>
  </section> -->

  <footer class="footer">
    <div class="container has-text-centered">
      <p>Template version — replace all <code>{{...}}</code> placeholders with your content.</p>
    </div>
  </footer>

  <!-- Local scripts -->
  <script src="./static/js/index.js"></script>
</body>
</html>

