<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="utf-8" />
  <meta name="description" content="Project / Paper description" />
  <meta name="keywords" content="navigation, dataset, accessibility" />
  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <title>Time-Aware Assistive Navigation</title>

  <!-- Local CSS (replace with your own files if needed) -->
  <link rel="stylesheet" href="./static/css/bulma.min.css" />
  <link rel="stylesheet" href="./static/css/index.css" />

  <meta property="og:site_name" content="{{PROJECT_NAME}}" />
  <meta property="og:type" content="website" />
  <meta property="og:title" content="{{TITLE}}" />

  <style>
    .video-and-annotations { display:flex; align-items:center; gap:1.5rem; flex-wrap:wrap; }
    .annotation-box { padding:12px; border-radius:6px; }
    .annotation-box.high-level { background:#6e6e6e; color:#fff; }
    .annotation-box.low-level { background:#e9e9e9; color:#111; }
    .annotation-title { font-size:1.05rem; font-weight:600; margin-bottom:0.4rem; }
    .publication-title { margin-bottom:0.5rem; }
    .publication-links .link-block { margin-right:0.5rem; display:inline-block; margin-bottom:0.4rem; }
    footer { margin-top:2.5rem; padding:1rem 0; }
  </style>
</head>
<body>
  <section class="hero">
    <div class="hero-body">
      <div class="container has-text-centered">
        <h1 class="title is-1 publication-title">Time-Aware Assistive Navigation</h1>
<!--         <div class="is-size-5 publication-authors">
          <span class="author-block">{{Anonymous Submission}}</span>
          <div class="brmod"></div>
        </div> -->

<!--         <div class="publication-links" style="margin-top:1rem;">
          <span class="link-block">
            <a href="./resources/paper.pdf" class="button is-normal is-rounded is-dark">
              <span>Paper</span>
            </a>
          </span>
          <span class="link-block">
            <a href="#video_section" class="button is-normal is-rounded is-dark">
              <span>Video</span>
            </a>
          </span>
          <span class="link-block">
            <a href="./resources/visualization.zip" class="button is-normal is-rounded is-dark">
              <span>Visualization</span>
            </a>
          </span>
          <span class="link-block">
            <a href="./resources/data_archive.zip" class="button is-normal is-rounded is-dark">
              <span>Data</span>
            </a>
          </span>
        </div> -->
      </div>
    </div>
  </section>

    
  <!-- Abstract -->
  <section class="section">
    <div class="container">
        <div class="columns is-centered has-text-centered">
            <div class="column is-two-thirds">
                <h2 class="title is-3">Abstract</h2>
                <div class="content has-text-justified">
                    <p>
                      Our TIMELI task goes beyond standard video-based summarization and indoor navigation benchmarks for MLLMs—we uniquely emphasize spatio-temporal coherence and planning in delivering continual language instructions under dynamic, end-user interactive settings. 
                      On this website, we provide additional details on: 
                      (1) <strong>benchmark construction</strong> and formulation, including data generation, annotation, and quality validation processes; 
                      (2) full <strong>prompt structure</strong> and experimental details, including metric definitions; 
                      (3) model ablations and additional <strong>results</strong>, including a preliminary validation in <strong>closed-loop real-world</strong> settings, 
                      leveraging a smartphone-based app instantiation (to be open-sourced) and generalization ablations on related <strong>real-world tasks</strong> to further study the benchmark's impact; and 
                      (4) additional <strong>qualitative results</strong>, including both success and failure cases.</p>
                </div>
            </div>
        </div>
    </div>
  </section>

  <!-- Video + Annotations -->
  <section class="section">
    <div class="container">
      <div class="columns is-centered">
        <div class="column is-two-thirds">

          <div class="columns video-and-annotations">
            <div class="column is-full">
              <div class="vis-video">
                <video controls muted loop style="width:100%">
                  <source src="./resources/TIMELI_supp.mp4" type="video/mp4" />
                </video>
              </div>
            </div>

            <div class="column is-full">
<!--               <h3 class="annotation-title">High-Level Annotation</h3>
              <div class="annotation-box high-level"> -->
                <div class="content">
                  <p>
                  This video is divided into four key segments:
                  <ul>
                    <li><strong>0:03 - 0:39:</strong> TIMELI Benchmark (with GT).</li>
                    <li><strong>0:40 - 1:12:</strong> ChatGPT-4o Performance.</li>
                    <li><strong>1:13 - 2:05:</strong> Finetuned Model Performance on Simulation Videos.</li>
                    <li><strong>2:06 - 2:57:</strong> Qualitative Results on Real-World Videos.</li>
                  </ul>
                </p>
                </div>
              </div>
            </div>
          </div>

          <!-- Duplicate above block for more videos/annotations -->

        </div>
      </div>
    </div>
  </section>


  <!-- Data / Statistics -->
<!--   <section class="section">
    <div class="container has-text-centered">
      <h2 class="title is-4">Data / Statistics</h2>
      <div class="column">
        <img src="./resources/data_statistic.png" alt="data statistics" style="max-width:100%;" />
      </div>
    </div>
  </section> -->

  <!-- Video section -->
<!--   <section id="video_section" class="section">
    <div class="container has-text-centered">
      <h2 class="title is-3">Video</h2>
      <div class="publication-video">
        <video controls style="max-width:100%;">
          <source src="./resources/supplementary_video.mp4" type="video/mp4" />
          Your browser does not support the video tag.
        </video>
      </div>
    </div>
  </section> -->

  <!-- Table of Contents -->
  <section class="section">
    <div class="container toc">
      <h2 class="title is-3">Table of Contents</h2>
      <ul>
        <li><a href="#sec1">1 TIMELI Benchmark Details</a>
          <ul>
            <li><a href="#sec1-1">1.1 Informed Instruction Design</a></li>
            <li><a href="#sec1-2">1.2 Adapting CARLA to Realistic Assistive Navigation Scenarios</a></li>
            <li><a href="#sec1-3">1.3 Real-World Video Annotation Process</a></li>
          </ul>
        </li>
        <li><a href="#sec2">2 Experimental Settings</a>
          <ul>
            <li><a href="#sec2-1">2.1 Prompt and Input Settings</a></li>
            <li><a href="#sec2-2">2.2 Metric Definitions</a></li>
            <li><a href="#sec2-3">2.3 Realistic Closed-Loop Evaluation with Noisy Conditions</a></li>
          </ul>
        </li>
        <li><a href="#sec3">3 Additional Experimental Analysis</a>
          <ul>
            <li><a href="#sec3-1">3.1 Results Summary and Error Intervals</a></li>
            <li><a href="#sec3-2">3.2 Model Components Ablation</a></li>
            <li><a href="#sec3-3">3.3 Safety-Critical Timing Analysis</a></li>
            <li><a href="#sec3-4">3.4 Inference Time</a></li>
            <li><a href="#sec3-5">3.5 Model Generalization to BDD</a></li>
            <li><a href="#sec3-6">3.6 Preliminary Smartphone-based Evaluation</a></li>
          </ul>
        </li>
        <li><a href="#sec4">4 Additional Qualitative Results</a></li>
      </ul>
    </div>
  </section>

  <!-- Sections -->
  <section class="section">
    <div class="container">
      <h2 id="sec1" class="title is-2">1 TIMELI Benchmark Details</h2>
      <h3 id="sec1-1" class="title is-4">1.1 Informed Instruction Design</h3>
        <figure class="image" style="max-width:800px; margin:0 auto; text-align:center;">
          <img src="./resources/supp1.png" 
               style="width:100%; height:auto; display:block; margin:0 auto;" 
               alt="Pedestrian Paths"> 
          <figcaption style="margin-top:0.5rem; font-size:1rem; color:#555;">
            <strong>Figure 1: Extracting Layout Details for Modified Pedestrian Paths.</strong>  We extract walkable and
            sidewalk space in the map, and employ it to sample waypoints nearby buildings and curbs. In
            the figure, red denotes sidewalk space and white and green denotes road or obstructed space. The
            sampled waypoints are then provided as intermediate goals to guide path planning for spawned
            pedestrians during data collection. This ensures a realistic and challenging benchmark.
          </figcaption>
        </figure>

      <figure class="image" style="max-width:800px; margin:0 auto; text-align:center;">
          <img src="./resources/supp2_statistics.png" 
               style="width:100%; height:auto; display:block; margin:0 auto;" 
               alt="Pedestrian Paths"> 
          <figcaption style="margin-top:0.5rem; font-size:1rem; color:#555;">
            <strong>Figure 2: Benchmark Statistics</strong> Left: distribution of events, categorized by whether the user is at
            an intersection. Right: keywords from instructional tasks, derived through in-situ study and trained
            mobility guides interviews.
          </figcaption>
        </figure>

      <figure class="image" style="max-width:800px; margin:0 auto; text-align:center;">
          <img src="./resources/supp3_annotation_interface.png" 
               style="width:100%; height:auto; display:block; margin:0 auto;" 
               alt="Pedestrian Paths"> 
          <figcaption style="margin-top:0.5rem; font-size:1rem; color:#555;">
            <strong>Figure 3: Annotation Interface for YouTube Videos</strong> We collect a diverse set of U.S.-based YouTube 
            videos and manually annotate them through the interface in a multi-round validation process.
          </figcaption>
        </figure>

      <figure class="image" style="max-width:800px; margin:0 auto; text-align:center;">
          <img src="./resources/supp4_piechart.png" 
               style="width:100%; height:auto; display:block; margin:0 auto;" 
               alt="Pedestrian Paths"> 
          <figcaption style="margin-top:0.5rem; font-size:1rem; color:#555;">
            <strong>Figure 4: Radar Plot for Fine-tuned Models in Simulation.</strong> We leverage the proposed video-based
            assistive navigation task to evaluate multiple dimensions of instruction generation models. 
            By finetuning models on our TIMELI benchmark, we observe that existing models frequently make errors in
            our navigation task, including timing of instructions (Timing), clarity and brevity of communication
            (Conciseness), identification of the reason (Instruction Reason), appropriate silence in intersections
            (Silence in Junction), accuracy in detecting relevant obstacles (Object Precision), and correctness of
            direction (Instruction Direction).
          </figcaption>
        </figure>

       <figure class="image" style="max-width:800px; margin:0 auto; text-align:center;">
          <img src="./resources/supp5_smartphone.png" 
               style="width:100%; height:auto; display:block; margin:0 auto;" 
               alt="Pedestrian Paths"> 
          <figcaption style="margin-top:0.5rem; font-size:1rem; color:#555;">
            <strong>Figure 5: iPhone App Interface</strong> We validate our model in closed-loop, real-world settings using an
            iPhone app instantiation. The app displays the front-view image on the left and the planned path to
            the destination (via Google Maps) on the right. Instructions are shown at the top and also delivered
            through audio.
          </figcaption>
        </figure>

       <figure class="image" style="max-width:800px; margin:0 auto; text-align:center;">
          <img src="./resources/supp6_closedloop_vis.png" 
               style="width:100%; height:auto; display:block; margin:0 auto;" 
               alt="Pedestrian Paths"> 
          <figcaption style="margin-top:0.5rem; font-size:1rem; color:#555;">
            <strong>Figure 6: Closed-loop Visualization.</strong> We present success and failure cases from the closed-loop
            navigation evaluation. The blue line shows the intended route, and the orange line shows the path a
            follower walks based on the instructions of a TIMELI-trained model. For failure cases, the model
            demonstrates an ability to recover from deviations by issuing corrective instructions. However, the
            recovery process often incurs additional time, which can lead to failure to reach the destination within
            the allotted time budget.
          </figcaption>
        </figure>

      
        <p>...</p>

      <h3 id="sec1-2" class="title is-4">1.2 Adapting CARLA...</h3>
      <p>...</p>

      <h3 id="sec1-3" class="title is-4">1.3 Real-World Video Annotation Process</h3>
      <p>...</p>

      <h2 id="sec2" class="title is-2">2 Experimental Settings</h2>
      <h3 id="sec2-1" class="title is-4">2.1 Prompt and Input Settings</h3>
      <p>...</p>

      <h3 id="sec2-2" class="title is-4">2.2 Metric Definitions</h3>
      <p>...</p>

      <h3 id="sec2-3" class="title is-4">2.3 Realistic Closed-Loop Evaluation with Noisy Conditions</h3>
      <p>...</p>

      <h2 id="sec3" class="title is-2">3 Additional Experimental Analysis</h2>
      <h3 id="sec3-1" class="title is-4">3.1 Results Summary and Error Intervals</h3>
      <p>...</p>

      <h3 id="sec3-2" class="title is-4">3.2 Model Components Ablation</h3>
      <p>...</p>

      <h3 id="sec3-3" class="title is-4">3.3 Safety-Critical Timing Analysis</h3>
      <p>...</p>

      <h3 id="sec3-4" class="title is-4">3.4 Inference Time</h3>
      <p>...</p>

      <h3 id="sec3-5" class="title is-4">3.5 Model Generalization to BDD</h3>
      <p>...</p>

      <h3 id="sec3-6" class="title is-4">3.6 Preliminary Smartphone-based Evaluation</h3>
      <p>...</p>

      <h2 id="sec4" class="title is-2">4 Additional Qualitative Results</h2>
      <p>...</p>
    </div>
  </section>

  <!-- Acknowledgments -->
<!--   <section class="section">
    <div class="container has-text-centered">
      <h2 class="title is-3">Acknowledgments</h2>
      <div class="content has-text-justified">
        <p>{{ACKNOWLEDGMENTS_PLACEHOLDER}}</p>
      </div>
    </div>
  </section> -->

  <!-- BibTeX -->
<!--   <section class="section">
    <div class="container has-text-centered">
      <h2 class="title is-3">BibTeX</h2>
      <pre><code>
@inproceedings{placeholder202X,
  title={{TITLE}},
  author={{AUTHORS_PLACEHOLDER}},
  booktitle={{CONFERENCE_PLACEHOLDER}},
  year={{YEAR_PLACEHOLDER}}
}
      </code></pre>
    </div>
  </section> -->

  <footer class="footer">
    <div class="container has-text-centered">
      <p>Template version — replace all <code>{{...}}</code> placeholders with your content.</p>
    </div>
  </footer>

  <!-- Local scripts -->
  <script src="./static/js/index.js"></script>
</body>
</html>

